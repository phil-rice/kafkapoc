#!/usr/bin/env python3
"""
replay_to_eventhub.py

Reads input scan XML files generated by generate_rm_full_dataset.py (or similar),
and replays them into an Azure Event Hub, preserving original inter-event time gaps
but compressed by a user-defined factor.

Strategy:
 - Files are expected to be named like: tracking_<uniqueItemId>_<scanIndex>.xml
 - The script groups files by scanIndex (1..4).
 - For scanIndex in ascending order:
     * load metadata for all files in that group (path and parsed scanTimestamp)
     * sort by scanTimestamp
     * send events in that order
     * before sending each event: compute delta = event_ts - last_original_ts (use 0 if none)
       sleep for (delta_seconds / compression_factor)
     * update last_original_ts = event_ts after sending
 - This preserves global timeline replay while only loading group-by-group.

Note:
 - The script sends the entire XML content as the event body.
 - Optionally you can throttle batch sizes or add custom properties (e.g., parcel id).
"""

import argparse
import os
import re
import time
import xml.etree.ElementTree as ET
from datetime import datetime, timezone, timedelta
from typing import List, Tuple

from azure.eventhub import EventData, EventHubProducerClient

FILENAME_PATTERN = re.compile(r"tracking_(?P<uid>[^_]+)_(?P<idx>\d+)\.xml", re.IGNORECASE)

# Helper to parse scanTimestamp from input XML
def extract_scan_timestamp(xml_bytes: bytes) -> datetime:
    """
    Extracts <scanTimestamp>...+01:00</scanTimestamp> or similar from the xml bytes.
    Returns a timezone-aware datetime in UTC.
    """
    try:
        root = ET.fromstring(xml_bytes)
    except ET.ParseError as e:
        raise ValueError(f"XML parse error: {e}")

    # find scanTimestamp element anywhere under manualScan
    # search by tag ending with 'scanTimestamp' to be resilient to namespaces
    scan_ts_elem = None
    for elem in root.iter():
        if elem.tag.endswith("scanTimestamp"):
            scan_ts_elem = elem
            break
    if scan_ts_elem is None or (scan_ts_elem.text is None):
        raise ValueError("scanTimestamp not found in XML")

    ts_text = scan_ts_elem.text.strip()

    # expect format like: 2025-10-07T03:00:09+01:00
    # Parse with offset aware parsing
    # Python's fromisoformat handles the offset if in Python 3.7+
    dt = None
    try:
        dt = datetime.fromisoformat(ts_text)
    except Exception:
        # fallback: try to remove colon in tz +0100
        try:
            if ts_text.endswith("Z"):
                dt = datetime.fromisoformat(ts_text.replace("Z", "+00:00"))
            else:
                # e.g. 2025-10-07T03:00:09+01:00
                dt = datetime.strptime(ts_text, "%Y-%m-%dT%H:%M:%S%z")
        except Exception as e:
            raise ValueError(f"Could not parse timestamp '{ts_text}': {e}")

    # convert to UTC for consistent delta calculations
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    dt_utc = dt.astimezone(timezone.utc)
    return dt_utc

def list_input_files(inputs_dir: str) -> List[str]:
    files = []
    for root, _, filenames in os.walk(inputs_dir):
        for fn in filenames:
            if fn.lower().endswith(".xml") and fn.lower().startswith("tracking_"):
                files.append(os.path.join(root, fn))
    return files

def group_files_by_index(files: List[str]) -> dict:
    groups = {}
    for f in files:
        m = FILENAME_PATTERN.search(os.path.basename(f))
        if not m:
            continue
        idx = int(m.group("idx"))
        groups.setdefault(idx, []).append(f)
    return groups

def prepare_group_metadata(filepaths: List[str]) -> List[Tuple[str, datetime]]:
    meta = []
    for fp in filepaths:
        with open(fp, "rb") as fh:
            data = fh.read()
        try:
            ts = extract_scan_timestamp(data)
        except Exception as e:
            raise RuntimeError(f"Error parsing timestamp in {fp}: {e}")
        meta.append((fp, ts))
    # sort by timestamp ascending
    meta.sort(key=lambda x: x[1])
    return meta

def send_event(event_hub_conn_str: str, eventhub_name: str, body_bytes: bytes, properties: dict = None):
    # We'll use a short-lived producer per batch or reuse a global producer (opt for global reuse for performance).
    # This wrapper exists if we later want to add per-event diagnostics.
    return EventData(body_bytes)

def replay_to_eventhub(
    inputs_dir: str,
    eventhub_connection_str: str,
    eventhub_name: str,
    compression: float = 60.0,
    batch_size: int = 50,
    dry_run: bool = False
):
    """
    inputs_dir: directory containing input XMLs (or zipped inputs extracted)
    eventhub_connection_str: Azure Event Hubs connection string (namespace-level or policy-level with Send)
    eventhub_name: target event hub name
    compression: time compression factor (original_seconds / compression = replay_sleep_seconds)
                 e.g., compression=60 means 1 minute of original time -> 1 second of sleep
    batch_size: number of EventData per send batch
    dry_run: if True, do not send to event hub; instead print progress
    """
    all_files = list_input_files(inputs_dir)
    if not all_files:
        raise RuntimeError(f"No input files found in {inputs_dir}")

    groups = group_files_by_index(all_files)
    # We'll process groups in numeric order (1..4)
    group_indices = sorted(groups.keys())

    # Establish EventHub producer (if not dry_run)
    producer = None
    if not dry_run:
        producer = EventHubProducerClient.from_connection_string(conn_str=eventhub_connection_str, eventhub_name=eventhub_name)

    last_original_ts = None  # datetime in UTC of last sent event

    # We'll reuse one producer send per group in streaming batches
    for idx in group_indices:
        print(f"Processing group (scanIndex) = {idx}, files = {len(groups[idx])}")
        meta = prepare_group_metadata(groups[idx])  # list of (filepath, timestamp_utc) sorted by timestamp

        # iterate sorted events in this group
        batch_events = []
        for fp, event_ts in meta:
            if last_original_ts is None:
                delta = timedelta(0)
            else:
                delta = event_ts - last_original_ts
                if delta.total_seconds() < 0:
                    # In rare cases if timestamps are not strictly increasing across groups,
                    # avoid negative sleeps by setting delta=0 (preserve ordering)
                    delta = timedelta(0)

            # compute sleep time based on compression factor
            sleep_seconds = delta.total_seconds() / compression if compression > 0 else 0.0
            if sleep_seconds > 0:
                print(f"Sleeping {sleep_seconds:.3f}s to mimic {delta.total_seconds():.1f}s original gap")
                time.sleep(sleep_seconds)

            # load XML payload
            with open(fp, "rb") as fh:
                body = fh.read()

            # build EventData with optional properties (e.g., filename)
            ed = EventData(body)
            # add application properties to help downstream consumers
            ed.properties = {
                "source_file": os.path.basename(fp),
                "original_timestamp_utc": event_ts.isoformat(),
                "scan_index": idx
            }

            if dry_run:
                print(f"[DRY] Would send {os.path.basename(fp)} at replay_time={datetime.utcnow().isoformat()} (original={event_ts.isoformat()})")
            else:
                # send in batches
                batch_events.append(ed)
                if len(batch_events) >= batch_size:
                    # send batch
                    with producer:
                        # create a batch and add events (sdk will raise if event too large)
                        # but better to use producer.create_batch and add one by one
                        batch = producer.create_batch()
                        for ev in batch_events:
                            try:
                                batch.add(ev)
                            except ValueError:
                                # batch full; send current batch, create new batch
                                producer.send_batch(batch)
                                batch = producer.create_batch()
                                batch.add(ev)
                        if len(batch) > 0:
                            producer.send_batch(batch)
                    batch_events = []

            # update last_original_ts after the event is considered "sent"
            last_original_ts = event_ts

        # after finishing group, flush remaining batch_events
        if not dry_run and batch_events:
            with producer:
                batch = producer.create_batch()
                for ev in batch_events:
                    try:
                        batch.add(ev)
                    except ValueError:
                        producer.send_batch(batch)
                        batch = producer.create_batch()
                        batch.add(ev)
                if len(batch) > 0:
                    producer.send_batch(batch)
            batch_events = []

        print(f"Finished group {idx}")

    print("Replay complete.")

# ---------------------------
# CLI
# ---------------------------

def parse_args():
    import argparse
    p = argparse.ArgumentParser(description="Replay input scan XMLs into Azure Event Hub while preserving time gaps.")
    p.add_argument("--inputs-dir", required=True, help="Directory containing input XMLs (extracted from zip).")
    p.add_argument("--eventhub-conn", required=False, help="Azure Event Hub connection string (if dry-run omit).")
    p.add_argument("--eventhub-name", required=False, help="Event Hub name (if dry-run omit).")
    p.add_argument("--compression", type=float, default=60.0, help="Time compression factor (original_sec / compression = sleep_sec).")
    p.add_argument("--batch-size", type=int, default=50, help="Number of events to group into each EventHub batch send.")
    p.add_argument("--dry-run", action="store_true", help="If set, do not actually send events; print actions instead.")
    return p.parse_args()

if __name__ == "__main__":
    args = parse_args()

    if args.dry_run:
        print("DRY RUN mode: no events will be sent.")
    else:
        if not args.eventhub_conn or not args.eventhub_name:
            raise SystemExit("When not in dry-run, --eventhub-conn and --eventhub-name are required.")

    replay_to_eventhub(
        inputs_dir=args.inputs_dir,
        eventhub_connection_str=args.eventhub_conn,
        eventhub_name=args.eventhub_name,
        compression=args.compression,
        batch_size=args.batch_size,
        dry_run=args.dry_run
    )
